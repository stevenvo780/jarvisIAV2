#!/usr/bin/env python3
import os
import sys
import time
import logging
import threading
import argparse
from queue import Queue, Empty
from dotenv import load_dotenv
from concurrent.futures import ThreadPoolExecutor

PROJECT_ROOT = os.path.dirname(os.path.abspath(__file__))
SRC_DIR = os.path.join(PROJECT_ROOT, 'src')
sys.path.insert(0, PROJECT_ROOT)
sys.path.insert(0, SRC_DIR)

from src.utils.error_handler import setup_logging
from src.utils.audio_utils import AudioEffects
from src.utils.jarvis_state import JarvisState
from modules.text.terminal_manager import TerminalManager
from src.modules.orchestrator.model_orchestrator import ModelOrchestrator
from src.modules.embeddings.embedding_manager import EmbeddingManager
from src.modules.voice.whisper_handler import WhisperHandler
from src.utils.metrics_tracker import MetricsTracker, QueryTimer
from modules.system_monitor import SystemMonitor
from src.modules.text.text_handler import TextHandler
from src.modules.voice.audio_handler import AudioHandler
from src.modules.voice.tts_manager import TTSManager
from src.modules.storage_manager import StorageManager
from modules.actions import Actions
from modules.system.command_manager import CommandManager

setup_logging()

class Jarvis:
    def __init__(self):
        # Thread-safe state management
        self.state = JarvisState(
            running=True,
            voice_active=False,
            listening_active=False,
            error_count=0,
            max_errors=5
        )
        self.input_queue = Queue()
        self.executor = ThreadPoolExecutor(max_workers=2)
        load_dotenv()
        try:
            self.terminal = TerminalManager()
            self.terminal.print_header("Starting Jarvis AI Assistant (Multi-GPU + RAG)")
            
            self.system_monitor = SystemMonitor()
            self.terminal.print_success("System monitor initialized")
            
            self._initialize_tts()
            self.terminal.print_success("TTS initialized")
            
            self.storage = StorageManager()
            self.terminal.print_success("Storage initialized")
            
            self.audio_effects = AudioEffects()
            self.terminal.print_success("Audio effects initialized")
            
            # Inicializar m√©tricas y monitoreo
            self.metrics = MetricsTracker(
                log_path="logs/metrics.jsonl",
                enable_gpu_monitoring=True
            )
            self.terminal.print_success("Metrics tracker initialized")
            
            # Inicializar embeddings/RAG
            enable_rag = os.getenv('ENABLE_RAG', 'true').lower() == 'true'
            if enable_rag:
                try:
                    self.embeddings = EmbeddingManager(
                        model_name="models/embeddings/bge-m3",
                        device="cuda:1",
                        chroma_path="vectorstore/chromadb"
                    )
                    self.terminal.print_success("Embedding system (RAG) initialized")
                except Exception as e:
                    self.terminal.print_warning(f"Embeddings disabled: {e}")
                    self.embeddings = None
            else:
                self.terminal.print_status("RAG/Embeddings disabled (ENABLE_RAG=false)")
                self.embeddings = None
            
            self.text_handler = None
            self.command_manager = None
            self.model_manager = None
            self.orchestrator = None
            self.actions = None
            
            self._initialize_llm()
            self.terminal.print_success("LLM system initialized")
            
            self._async_audio_init()
            self.terminal.print_success("Voice system initialized")
            
            self._initialize_actions()
            self.terminal.print_success("Actions initialized")
            
            self.text_handler = TextHandler(
                terminal_manager=self.terminal,
                input_queue=self.input_queue,
                actions=self.actions,
                embeddings=self.embeddings
            )
            self.terminal.print_success("Text handler initialized")
        
            self.terminal.print_header("Initializing Jarvis")
            
            # Mostrar estad√≠sticas
            if hasattr(self, 'orchestrator'):
                stats = self.orchestrator.get_stats()
                self.terminal.print_success(f"GPUs: {stats['gpu_count']} | Models loaded: {stats['models_loaded']}")
            
            self.terminal.print_status("Jarvis Text Interface - Escribe 'help' para ver los comandos")
            self.terminal.update_prompt_state('NEUTRAL')
            self.audio_effects.play('startup')
        except Exception as e:
            self.audio_effects.play('error')
            self.terminal.print_error(f"Initialization error: {e}")
            sys.exit(1)


    def _initialize_actions(self):
        try:
            self.actions = Actions(
                tts=self.tts,
                state=self.state,
                audio_effects=self.audio_effects,
                audio_handler=self.audio 
            )
        except Exception as e:
            raise e

    def _initialize_tts(self):
        try:
            self.tts = TTSManager()
            self.state.set_voice_active(True)
        except Exception as e:
            self.terminal.print_warning(f"TTS initialization error: {e}")
            self.state.set_voice_active(False)
            raise e

    def _initialize_llm(self):
        try:
            # Usar ModelOrchestrator para Multi-GPU
            self.orchestrator = ModelOrchestrator(
                config_path="src/config/models.json"
            )
            self.model_manager = None
            self.terminal.print_success("ModelOrchestrator initialized (Multi-GPU)")
            
            # CommandManager usa el orchestrator
            self.command_manager = CommandManager(
                model_manager=self.orchestrator,
            )
        except Exception as e:
            raise e

    def _async_audio_init(self):
        enable_whisper = os.getenv('ENABLE_WHISPER', 'true').lower() == 'true'
        
        if not enable_whisper:
            self.terminal.print_status("‚å®Ô∏è Text mode - ENABLE_WHISPER=false")
            self.whisper = None
            self.audio = None
            self.state.set_listening_active(False)
            return
            
        try:
            logging.info("Iniciando inicializaci√≥n de audio...")
            
            # Intentar usar WhisperHandler optimizado
            try:
                self.whisper = WhisperHandler(
                    model_path="models/whisper/large-v3-turbo-ct2",
                    device="cuda",
                    device_index=1,
                    compute_type="int8"
                )
                self.terminal.print_success("WhisperHandler initialized")
            except Exception as e:
                self.terminal.print_warning(f"Whisper fallback to CPU: {e}")
                try:
                    self.whisper = WhisperHandler(device="cpu")
                except Exception as e2:
                    self.terminal.print_error(f"Whisper CPU fallback also failed: {e2}")
                    self.whisper = None
            
            # AudioHandler (wrapper para reconocimiento de voz)
            if self.whisper:
                self.audio = AudioHandler(
                    terminal_manager=self.terminal,
                    tts=self.tts,
                    state=self.state,
                    input_queue=self.input_queue,
                    whisper_handler=self.whisper
                )
                self.state.set_listening_active(True)
            else:
                self.audio = None
                self.state.set_listening_active(False)
                self.terminal.print_warning("‚å®Ô∏è Text mode only - Whisper not available")
        except Exception as e:
            self.terminal.print_warning(f"‚å®Ô∏è Text mode only: {e}")
            self.state.set_listening_active(False)
            self.terminal.print_error(f"Error inicializando audio: {e}")
            self.audio_effects.play('error')
            logging.error(f"Audio init error: {e}")
            self.audio = None

    def _handle_critical_error(self, message):
        logging.critical(message)
        self.terminal.print_error(message)
        self.audio_effects.play('error')
        if self.state.increment_errors():
            self.terminal.print_error("Max error limit reached")
            self._shutdown_system()

    def _system_monitor(self):
        while self.state.is_running():
            try:
                status = self.system_monitor.check_system_health()
                if all(status.values()):
                    self.state.reset_errors()
                else:
                    self.state.increment_errors()
                    f = [k.replace('_ok','') for k,v in status.items() if not v]
                    logging.warning(f"Compromised: {f}")
                    time.sleep(5)
            except Exception as e:
                logging.error(f"Monitor error: {str(e)}")
                if self.state.get_error_count() >= self.state.max_errors:
                    sys.exit(1)
            time.sleep(1)

    def _process_inputs(self):
        try:
            t, content = self.input_queue.get_nowait()
            
            if content.strip():
                if hasattr(self, 'tts'):
                    self.tts.stop_speaking()

                if self.actions:
                    message, is_command = self.actions.handle_command(content)
                    if is_command == True:
                        if message:
                            self.terminal.print_response(message, "system")
                            self.storage.add_interaction({
                                "query": content,
                                "response": message,
                                "model": "system",
                                "type": "system-command"
                            })
                        self.terminal.update_prompt_state('NEUTRAL')
                        self.input_queue.task_done()
                        return
                
                self.terminal.update_prompt_state('PROCESSING')
                if self.command_manager:
                    response, response_type = self.command_manager.process_input(content)
                    if response and response_type in ["command", "error"]:
                        self.terminal.print_response(response, "system" if response_type == "command" else "error")
                        self.storage.add_interaction({
                            "query": content,
                            "response": response,
                            "model": "system" if response_type == "command" else "error",
                            "type": f"system-{response_type}"
                        })
                        self.input_queue.task_done()
                        
                        self.terminal.update_prompt_state('SPEAKING')
                        self.tts.speak(response)
                        self.terminal.update_prompt_state('NEUTRAL')
                        return
                
                self.terminal.update_prompt_state('THINKING')
                try:
                    # Usar orchestrator con m√©tricas y contexto RAG
                    if self.orchestrator:
                        # Calcular dificultad (simple heur√≠stica)
                        difficulty = self._estimate_difficulty(content)
                        
                        # Obtener contexto RAG si est√° disponible
                        rag_context = ""
                        if self.embeddings:
                            rag_context = self.embeddings.get_context_for_query(content, max_context=3)
                        
                        # Enriquecer query con contexto
                        enriched_query = content
                        if rag_context:
                            enriched_query = f"Contexto relevante:\n{rag_context}\n\nPregunta: {content}"
                        
                        # Consultar con m√©tricas
                        if self.metrics:
                            with QueryTimer(self.metrics, content, "auto", difficulty):
                                response, model_name = self.orchestrator.get_response(
                                    query=enriched_query,
                                    difficulty=difficulty
                                )
                        else:
                            response, model_name = self.orchestrator.get_response(
                                query=enriched_query,
                                difficulty=difficulty
                            )
                        
                        # Guardar en RAG para memoria de largo plazo
                        if self.embeddings:
                            self.embeddings.add_interaction(
                                query=content,
                                response=response,
                                model=model_name,
                                difficulty=difficulty
                            )
                    else:
                        # V1: Legacy model_manager
                        # aqui no se guarda interaccion por que ya esta dentro del get_response
                        response, model_name = self.model_manager.get_response(content)
                    
                    self.terminal.print_response(response, model_name)
                    if (t == 'voice' or self.state.voice_active) and hasattr(self, 'tts'):
                        self.tts.speak(response)
                except Exception as e:
                    self.terminal.print_error(f"Error del modelo: {e}")
                    self.storage.add_interaction({
                        "query": content,
                        "response": str(e),
                        "model": "error",
                        "type": "system-error"
                    })
                
            self.terminal.update_prompt_state('NEUTRAL')
            self.input_queue.task_done()
            
        except Empty:
            pass
        except Exception as e:
            self._handle_critical_error(f"Error processing input: {str(e)}")

    def _process_inputs_loop(self):
        while self.state.is_running():
            self._process_inputs()
            time.sleep(0.1)
        self._shutdown_system()

    def _estimate_difficulty(self, query: str) -> int:
        """
        Estima la dificultad de una query (0-100).
        Heur√≠stica simple basada en longitud, palabras clave, y complejidad.
        """
        query_lower = query.lower()
        difficulty = 30  # Base
        
        # Longitud
        if len(query) > 200:
            difficulty += 20
        elif len(query) > 100:
            difficulty += 10
        
        # Palabras clave complejas
        complex_keywords = [
            'explica', 'analiza', 'compara', 'teor√≠a', 'cient√≠fico', 'matem√°tica',
            'complejo', 'detallado', 'profundo', 't√©cnico', 'algoritmo', 'f√≠sica',
            'qu√≠mica', 'biolog√≠a', 'filosof√≠a', 'razona', 'demuestra', 'deriva'
        ]
        for keyword in complex_keywords:
            if keyword in query_lower:
                difficulty += 10
                break
        
        # Palabras clave simples
        simple_keywords = ['hola', 'qu√© tal', 'gracias', 'adi√≥s', 's√≠', 'no']
        for keyword in simple_keywords:
            if keyword in query_lower:
                difficulty = max(10, difficulty - 20)
                break
        
        return min(100, max(10, difficulty))

    def _shutdown_system(self, signum=None, frame=None):
        self.terminal.print_status("Shutting down...")
        try:
            self.state.set_running(False)
            self.terminal.print_warning("Shutdown signal received...")
            
            # Mostrar estad√≠sticas finales
            if self.metrics:
                self.terminal.print_header("Session Statistics")
                self.metrics.print_stats()
            
            if hasattr(self, 'executor'):
                self.executor.shutdown(wait=False)
            if hasattr(self, 'monitor_thread'):
                self.monitor_thread.join(timeout=2)
            if hasattr(self, 'processor_thread'):
                self.processor_thread.join(timeout=2)
            if hasattr(self, 'text_handler'):
                self.text_handler.stop()
            if hasattr(self, 'tts'):
                self.tts.cleanup()
            
            # Cleanup orchestrator
            if hasattr(self, 'orchestrator') and self.orchestrator:
                self.terminal.print_status("Unloading models...")
                # Python limpiar√° los modelos en exit
                del self.orchestrator
            
            if hasattr(self, 'model_manager') and self.model_manager:
                del self.model_manager
            
            # V2: Cleanup embeddings
            if hasattr(self, 'embeddings') and self.embeddings:
                stats = self.embeddings.get_statistics()
                self.terminal.print_success(f"RAG: {stats['total_memories']} memories saved")
                del self.embeddings
            
            if hasattr(self, 'audio') and self.audio:
                self.audio.cleanup()
            
            self.terminal.print_goodbye()
            
        except Exception as e:
            logging.error(f"Shutdown error: {str(e)}")
        finally:
            os._exit(0)

    def run(self):
        self.monitor_thread = threading.Thread(target=self._system_monitor, daemon=True)
        self.processor_thread = threading.Thread(target=self._process_inputs_loop, daemon=True)
        self.monitor_thread.start()
        self.processor_thread.start()
        
        self.text_handler.run_interactive()
    
    def process_command(self, query: str) -> str:
        """
        Process a single command/query and return the response
        Used for non-interactive mode
        
        Args:
            query: User query/command
            
        Returns:
            Response string
        """
        try:
            print(f"\nüîµ Query: {query}")
            
            # Check if it's a system command
            if self.actions:
                message, is_command = self.actions.handle_command(query)
                if is_command == True:
                    if message:
                        print(f"\nüü¢ System: {message}")
                        return message
                    return ""
            
            # Check command manager
            if self.command_manager:
                response, response_type = self.command_manager.process_input(query)
                if response and response_type in ["command", "error"]:
                    print(f"\nüü¢ System: {response}")
                    return response
            
            # Process with LLM orchestrator
            if self.orchestrator:
                # Calcular dificultad
                difficulty = self._estimate_difficulty(query)
                
                # Obtener contexto RAG si est√° disponible
                rag_context = ""
                if self.embeddings:
                    rag_context = self.embeddings.get_context_for_query(query, max_context=3)
                
                # Enriquecer query con contexto
                enriched_query = query
                if rag_context:
                    enriched_query = f"Contexto relevante:\n{rag_context}\n\nPregunta: {query}"
                
                print("\n‚è≥ Processing...")
                
                # Consultar
                response, model_name = self.orchestrator.get_response(
                    query=enriched_query,
                    difficulty=difficulty
                )
                
                # Guardar en RAG
                if self.embeddings:
                    self.embeddings.add_interaction(
                        query=query,
                        response=response,
                        model=model_name,
                        difficulty=difficulty
                    )
                
                print(f"\nüü¢ {model_name}: {response}")
                return response
            else:
                error_msg = "No hay modelos disponibles. Configura claves API o descarga modelos locales."
                print(f"\nüî¥ {error_msg}")
                return error_msg
            
        except Exception as e:
            error_msg = f"Error processing command: {e}"
            logging.error(error_msg)
            print(f"\nüî¥ {error_msg}")
            return error_msg

def parse_arguments():
    """Parse command line arguments"""
    parser = argparse.ArgumentParser(
        description="Jarvis AI Assistant - Multi-GPU LLM System",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # Interactive mode (default)
  python main.py
  
  # Command mode (single query)
  python main.py --query "¬øCu√°l es la capital de Francia?"
  
  # Command mode with specific model
  python main.py --query "Explica la teor√≠a de la relatividad" --model llama-70b
  
  # Show available models
  python main.py --list-models
        """
    )
    
    parser.add_argument(
        '-q', '--query',
        type=str,
        help='Execute a single query and exit (non-interactive mode)'
    )
    
    parser.add_argument(
        '-m', '--model',
        type=str,
        help='Specify model to use (e.g., llama-70b, qwen-32b, gpt-4o-mini)'
    )
    
    parser.add_argument(
        '--list-models',
        action='store_true',
        help='List available models and exit'
    )
    
    parser.add_argument(
        '--stats',
        action='store_true',
        help='Show system statistics and exit'
    )
    
    return parser.parse_args()

if __name__ == "__main__":
    args = parse_arguments()
    
    jarvis = Jarvis()
    
    try:
        # List models mode
        if args.list_models:
            print("\n" + "="*70)
            print("AVAILABLE MODELS")
            print("="*70)
            
            if hasattr(jarvis, 'orchestrator'):
                stats = jarvis.orchestrator.get_stats()
                print(f"\nüìä GPU Count: {stats['gpu_count']}")
                print(f"üì¶ Models Loaded: {len(stats['loaded_models'])}")
                
                for gpu_info in stats['gpus']:
                    print(f"\nüéÆ {gpu_info['name']} (GPU {gpu_info['id']})")
                    print(f"   VRAM: {gpu_info['vram_used_mb']}MB / {gpu_info['vram_total_mb']}MB ({gpu_info['vram_percent']}%)")
                
                print("\nüìã Configured Models:")
                # List all models from config
                import json
                with open('src/config/models.json', 'r') as f:
                    config = json.load(f)
                    for model_id, model_info in config.get('models', {}).items():
                        status = "‚úÖ LOADED" if model_id in stats['loaded_models'] else "‚¨ú NOT LOADED"
                        print(f"  {status} {model_id}: {model_info['name']} (GPU {model_info['gpu_id']}, {model_info['vram_required']}MB)")
            
            sys.exit(0)
        
        # Stats mode
        if args.stats:
            print("\n" + "="*70)
            print("SYSTEM STATISTICS")
            print("="*70)
            
            if hasattr(jarvis, 'orchestrator'):
                stats = jarvis.orchestrator.get_stats()
                print(f"\nüéÆ GPUs: {stats['gpu_count']}")
                for gpu_info in stats['gpus']:
                    print(f"  GPU {gpu_info['id']}: {gpu_info['name']}")
                    print(f"    VRAM: {gpu_info['vram_used_mb']}MB / {gpu_info['vram_total_mb']}MB")
            
            if hasattr(jarvis, 'embeddings') and jarvis.embeddings:
                emb_stats = jarvis.embeddings.get_statistics()
                print(f"\nüíæ Memories: {emb_stats.get('total_memories', 0)}")
            
            sys.exit(0)
        
        # Command mode
        if args.query:
            # TODO: Support model selection with args.model
            response = jarvis.process_command(args.query)
            sys.exit(0)
        
        # Interactive mode (default)
        jarvis.run()
        
    except KeyboardInterrupt:
        pass
    except Exception as e:
        jarvis._shutdown_system()
        logging.error(f"Fatal error: {e}")
