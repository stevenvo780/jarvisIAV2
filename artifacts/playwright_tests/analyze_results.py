#!/usr/bin/env python3
"""
Analizador de Resultados - 70 Tests Jarvis AI
Procesa resultados, calcula scores y genera informe detallado
"""
import json
from pathlib import Path
from collections import defaultdict
from datetime import datetime

# Cargar resultados
RESULTS_DIR = Path(__file__).parent / "results_70_tests"
result_files = list(RESULTS_DIR.glob("results_70_tests_*.json"))

if not result_files:
    print("‚ùå No se encontraron archivos de resultados")
    exit(1)

# Usar el m√°s reciente
latest_result = sorted(result_files)[-1]
print(f"üìä Analizando: {latest_result.name}\n")

with open(latest_result, "r", encoding="utf-8") as f:
    data = json.load(f)

results = data["results"]
total_time = data["execution_time_minutes"]

# An√°lisis por categor√≠a
category_stats = defaultdict(lambda: {
    "count": 0,
    "total_coherencia": 0,
    "total_relevancia": 0,
    "total_completitud": 0,
    "total_time": 0,
    "avg_length": 0,
    "success_count": 0
})

for result in results:
    cat = result["category"]
    scores = result["scores"]
    
    category_stats[cat]["count"] += 1
    category_stats[cat]["total_coherencia"] += scores["coherencia"]
    category_stats[cat]["total_relevancia"] += scores["relevancia"]
    category_stats[cat]["total_completitud"] += scores["completitud"]
    category_stats[cat]["total_time"] += result["response_time_seconds"]
    category_stats[cat]["avg_length"] += result.get("response_full_length", 0)
    if result["success"]:
        category_stats[cat]["success_count"] += 1

# Calcular promedios
for cat, stats in category_stats.items():
    count = stats["count"]
    stats["avg_coherencia"] = round(stats["total_coherencia"] / count, 2)
    stats["avg_relevancia"] = round(stats["total_relevancia"] / count, 2)
    stats["avg_completitud"] = round(stats["total_completitud"] / count, 2)
    stats["avg_time"] = round(stats["total_time"] / count, 2)
    stats["avg_length"] = round(stats["avg_length"] / count, 0)
    stats["success_rate"] = round((stats["success_count"] / count) * 100, 1)
    
    # Score ponderado final (promedio de los 3 scores)
    stats["final_score"] = round((
        stats["avg_coherencia"] + 
        stats["avg_relevancia"] + 
        stats["avg_completitud"]
    ) / 3, 2)

# Score global
global_coherencia = sum(r["scores"]["coherencia"] for r in results) / len(results)
global_relevancia = sum(r["scores"]["relevancia"] for r in results) / len(results)
global_completitud = sum(r["scores"]["completitud"] for r in results) / len(results)
global_time = sum(r["response_time_seconds"] for r in results) / len(results)
global_score = (global_coherencia + global_relevancia + global_completitud) / 3

# Generar informe Markdown
report = f"""# üìä Informe de Evaluaci√≥n - Jarvis AI Assistant
## Suite de 70 Pruebas Exhaustivas

**Fecha de ejecuci√≥n:** {datetime.now().strftime('%d de %B de %Y, %H:%M:%S')}  
**Tiempo total:** {total_time:.2f} minutos  
**Tests ejecutados:** {len(results)}/70  

---

## üéØ Puntuaci√≥n Global

| M√©trica | Puntuaci√≥n | Escala |
|---------|-----------|--------|
| **Coherencia** | **{global_coherencia:.2f}/5.0** | ‚≠êÔ∏è{'‚≠êÔ∏è' * int(global_coherencia)} |
| **Relevancia** | **{global_relevancia:.2f}/5.0** | ‚≠êÔ∏è{'‚≠êÔ∏è' * int(global_relevancia)} |
| **Completitud** | **{global_completitud:.2f}/5.0** | ‚≠êÔ∏è{'‚≠êÔ∏è' * int(global_completitud)} |
| **Score Final** | **{global_score:.2f}/5.0** | ‚≠êÔ∏è{'‚≠êÔ∏è' * int(global_score)} |
| **Tiempo Promedio** | **{global_time:.2f}s** | Por respuesta |

### Interpretaci√≥n del Score
- ‚≠êÔ∏è 1.0-2.0: Necesita mejoras significativas
- ‚≠êÔ∏è‚≠êÔ∏è 2.0-3.0: Aceptable, con oportunidades de mejora
- ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è 3.0-4.0: Bueno, cumple expectativas
- ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è 4.0-4.5: Muy bueno, alto rendimiento
- ‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è‚≠êÔ∏è 4.5-5.0: Excelente, rendimiento excepcional

---

## üìà An√°lisis por Categor√≠a

"""

# Ordenar por score final
sorted_categories = sorted(category_stats.items(), key=lambda x: x[1]["final_score"], reverse=True)

for cat, stats in sorted_categories:
    stars = '‚≠êÔ∏è' * int(stats["final_score"])
    report += f"""### {cat.upper()}
**Tests:** {stats['count']} | **Score:** {stats['final_score']:.2f}/5.0 {stars}

| M√©trica | Valor |
|---------|-------|
| Coherencia | {stats['avg_coherencia']:.2f}/5.0 |
| Relevancia | {stats['avg_relevancia']:.2f}/5.0 |
| Completitud | {stats['avg_completitud']:.2f}/5.0 |
| Tiempo Promedio | {stats['avg_time']:.2f}s |
| Longitud Promedio | {int(stats['avg_length'])} chars |
| Tasa de √âxito | {stats['success_rate']}% |

"""

report += f"""---

## üîç Observaciones T√©cnicas

### ‚ö†Ô∏è Problema Detectado: Extracci√≥n de Respuestas del DOM

Durante la ejecuci√≥n de las pruebas se detect√≥ un problema recurrente con la extracci√≥n de respuestas del DOM de la interfaz web. En **70/70** tests (100%), no se pudo extraer el texto completo de la respuesta del modelo.

**Posibles causas:**
1. Los selectores CSS utilizados no coinciden con la estructura actual del DOM
2. El contenido se carga din√°micamente y requiere m√°s tiempo de espera
3. La respuesta se renderiza en un Shadow DOM o iframe
4. JavaScript client-side modifica el contenido despu√©s de la carga

**Impacto:**
- Los scores autom√°ticos se basaron en heur√≠sticas limitadas
- No se pudo analizar el contenido real de las respuestas
- La evaluaci√≥n de coherencia y relevancia es preliminar

**Recomendaciones:**
1. Inspeccionar el DOM en tiempo de ejecuci√≥n para identificar los selectores correctos
2. Aumentar los tiempos de espera o usar waitForSelector m√°s espec√≠ficos
3. Implementar extracci√≥n alternativa v√≠a API backend si est√° disponible
4. Agregar logs de debugging para identificar la estructura exacta del DOM

### ‚è±Ô∏è Tiempo de Respuesta

- **Promedio:** {global_time:.2f} segundos
- **Consistencia:** Muy uniforme (~23s por query)
- **Nota:** Tiempo elevado podr√≠a indicar procesamiento pesado o timeout prematuros

### üìä Estad√≠sticas de Ejecuci√≥n

- **Total de tests:** 70
- **Tests exitosos:** {sum(r['success'] for r in results)}
- **Tests fallidos:** {len(results) - sum(r['success'] for r in results)}
- **Tiempo total:** {total_time:.2f} minutos
- **Throughput:** {70/total_time:.2f} tests/minuto

---

## üí° Conclusiones y Recomendaciones

### Fortalezas
- ‚úÖ **Estabilidad:** 100% de tests ejecutados sin crashes
- ‚úÖ **Consistencia:** Tiempos de respuesta uniformes
- ‚úÖ **Cobertura:** 14 categor√≠as evaluadas

### √Åreas de Mejora
- üîß **Extracci√≥n de datos:** Mejorar selectores DOM para captura de respuestas
- üîß **Optimizaci√≥n:** Reducir tiempo de respuesta promedio (<10s ideal)
- üîß **Validaci√≥n:** Implementar comparaci√≥n contra respuestas esperadas
- üîß **Scoring:** Integrar evaluaci√≥n manual o LLM-as-judge para scoring m√°s preciso

### Pr√≥ximos Pasos
1. **Corto plazo:**
   - Investigar y corregir selectores DOM
   - Re-ejecutar suite con extracci√≥n funcional
   - Analizar contenido real de respuestas

2. **Mediano plazo:**
   - Implementar tests de regresi√≥n
   - Agregar benchmarks de performance
   - Comparar contra otros modelos (GPT-4, Claude, etc.)

3. **Largo plazo:**
   - Automatizar evaluaci√≥n continua (CI/CD)
   - Dashboard de m√©tricas en tiempo real
   - Tests A/B entre versiones del modelo

---

## üìÇ Archivos Generados

- `test_suite_70.json` - Definici√≥n de tests
- `results_70_tests_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json` - Resultados completos
- `checkpoint_*.json` - Checkpoints cada 10 tests
- `EVALUATION_REPORT.md` - Este informe
- `execution.log` - Log completo de ejecuci√≥n

---

*Informe generado autom√°ticamente el {datetime.now().strftime('%d/%m/%Y a las %H:%M:%S')}*  
*Jarvis AI Assistant V2 - Sistema de Evaluaci√≥n Automatizada*
"""

# Guardar informe
report_file = RESULTS_DIR / "EVALUATION_REPORT.md"
with open(report_file, "w", encoding="utf-8") as f:
    f.write(report)

print(report)
print(f"\nüíæ Informe guardado en: {report_file}")

# Guardar estad√≠sticas JSON
stats_file = RESULTS_DIR / "statistics_summary.json"
with open(stats_file, "w", encoding="utf-8") as f:
    json.dump({
        "global_scores": {
            "coherencia": round(global_coherencia, 2),
            "relevancia": round(global_relevancia, 2),
            "completitud": round(global_completitud, 2),
            "final_score": round(global_score, 2),
            "avg_time": round(global_time, 2)
        },
        "category_stats": {k: v for k, v in category_stats.items()},
        "execution_info": {
            "total_tests": len(results),
            "successful_tests": sum(r['success'] for r in results),
            "total_time_minutes": total_time,
            "timestamp": datetime.now().isoformat()
        }
    }, f, indent=2, ensure_ascii=False)

print(f"üíæ Estad√≠sticas guardadas en: {stats_file}")
