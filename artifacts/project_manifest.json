{
  "project_name": "Jarvis IA V2",
  "analysis_date": "2025-11-06",
  "hardware": {
    "gpus": [
      {
        "id": 0,
        "model": "NVIDIA GeForce RTX 5070 Ti",
        "vram": "16GB",
        "compute_capability": "12.0",
        "driver": "580.95.05",
        "recommended_use": "Modelos grandes (70B-405B), inferencia principal"
      },
      {
        "id": 1,
        "model": "NVIDIA GeForce RTX 2060",
        "vram": "6GB",
        "compute_capability": "7.5",
        "driver": "580.95.05",
        "recommended_use": "Modelos pequeños (7B-14B), Whisper, embeddings"
      }
    ],
    "cpu": {
      "model": "AMD Ryzen 9 9950X3D",
      "cores": 16,
      "threads": 32,
      "recommended_use": "Embeddings, preprocessing, fallback inference"
    }
  },
  "current_state": {
    "python_version": "3.10.12",
    "models": {
      "local": {
        "name": "meta-llama/Llama-3.2-3B",
        "status": "OBSOLETO - Modelo muy pequeño para hardware disponible",
        "quantization": "4-bit",
        "issues": [
          "Subutiliza GPU de 16GB",
          "Respuestas limitadas por tamaño",
          "No aprovecha capacidades modernas"
        ]
      },
      "openai": {
        "name": "gpt-4o",
        "status": "ACTUALIZADO",
        "difficulty_range": "61-80"
      },
      "google": {
        "name": "gemini-2.0-flash-exp",
        "status": "ACTUALIZADO",
        "difficulty_range": "40-60"
      },
      "deepinfra": {
        "name": "deepseek-ai/DeepSeek-R1",
        "status": "PARCIAL - Usar API oficial",
        "difficulty_range": "81-100"
      }
    },
    "dependencies": {
      "torch": "2.5.1",
      "transformers": "4.48.1",
      "openai_whisper": "20240930 - OBSOLETO",
      "llama-cpp-python": ">=0.2.9 - Sin CUDA",
      "bitsandbytes": ">=0.41.1 - DESACTUALIZADO"
    },
    "issues": [
      "No usa llama.cpp server para optimización",
      "No tiene sistema de embeddings/RAG",
      "Whisper lento (no usa faster-whisper)",
      "Sin multi-GPU orchestration",
      "Sin monitoreo de VRAM",
      "Sin cache de modelos compartido",
      "Cuantización no optimizada para RTX 50xx"
    ]
  },
  "recommended_stack": {
    "inference_backends": [
      "vllm - Para modelos grandes en GPU1 (Llama 70B, Qwen 32B)",
      "llama-cpp-server - Para modelos medianos cuantizados (GGUF)",
      "transformers + flash-attention - Para flexibilidad",
      "CTranslate2/faster-whisper - Para ASR optimizado"
    ],
    "local_models": [
      {
        "name": "meta-llama/Llama-3.3-70B-Instruct",
        "size": "70B",
        "quantization": "AWQ/GPTQ 4-bit",
        "vram_required": "~14GB",
        "gpu": "RTX 5070 Ti",
        "use_case": "Respuestas complejas, razonamiento general"
      },
      {
        "name": "Qwen/Qwen2.5-32B-Instruct",
        "size": "32B",
        "quantization": "AWQ 4-bit",
        "vram_required": "~10GB",
        "gpu": "RTX 5070 Ti",
        "use_case": "Matemáticas, código, análisis técnico"
      },
      {
        "name": "meta-llama/Llama-3.2-8B-Instruct",
        "size": "8B",
        "quantization": "FP16",
        "vram_required": "~5GB",
        "gpu": "RTX 2060",
        "use_case": "Consultas rápidas, chat general"
      },
      {
        "name": "deepseek-ai/DeepSeek-R1-Distill-Qwen-14B",
        "size": "14B",
        "quantization": "GPTQ 4-bit",
        "vram_required": "~9GB",
        "gpu": "RTX 5070 Ti",
        "use_case": "Razonamiento paso a paso, matemáticas avanzadas"
      },
      {
        "name": "openai/whisper-large-v3-turbo",
        "size": "809M",
        "quantization": "INT8 CTranslate2",
        "vram_required": "~2GB",
        "gpu": "RTX 2060",
        "use_case": "Reconocimiento de voz optimizado"
      }
    ],
    "embeddings": [
      {
        "name": "BAAI/bge-m3",
        "size": "568M",
        "dimensions": 1024,
        "gpu": "RTX 2060 / CPU",
        "use_case": "Embeddings multilingües, RAG"
      },
      {
        "name": "intfloat/e5-mistral-7b-instruct",
        "size": "7B",
        "dimensions": 4096,
        "gpu": "RTX 2060",
        "use_case": "Embeddings de alta calidad para búsqueda"
      }
    ],
    "api_models": [
      {
        "name": "gpt-4o-mini",
        "provider": "OpenAI",
        "cost": "Bajo",
        "use_case": "Consultas complejas económicas"
      },
      {
        "name": "claude-3.5-sonnet",
        "provider": "Anthropic",
        "cost": "Medio",
        "use_case": "Razonamiento profundo, análisis"
      },
      {
        "name": "gemini-2.0-flash-thinking",
        "provider": "Google",
        "cost": "Bajo",
        "use_case": "Respuestas rápidas con razonamiento"
      },
      {
        "name": "deepseek-chat",
        "provider": "DeepSeek",
        "cost": "Muy Bajo",
        "use_case": "Matemáticas, código, razonamiento"
      }
    ]
  },
  "migration_priorities": [
    "1. Actualizar stack de inferencia (vLLM, llama.cpp-server)",
    "2. Implementar orquestador multi-GPU",
    "3. Migrar a faster-whisper",
    "4. Descargar y configurar modelos locales modernos",
    "5. Implementar sistema de embeddings + RAG",
    "6. Actualizar routing inteligente con métricas",
    "7. Añadir monitoreo y benchmarking",
    "8. Documentar y testear migración completa"
  ],
  "expected_improvements": {
    "latency_local": "3-5x más rápido (vLLM + modelos optimizados)",
    "quality": "Significativa (modelos 70B vs 3B actual)",
    "cost_reduction": "60-80% menos uso de APIs",
    "vram_utilization": "De ~3GB a 14-15GB (GPU1), ~5GB (GPU2)",
    "capabilities": "RAG, embeddings, multi-turn, razonamiento mejorado"
  }
}
