/mnt/DATA/repos/Personal/jarvisIAV2/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:942: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.
  warnings.warn(
/mnt/DATA/repos/Personal/jarvisIAV2/.venv/lib/python3.10/site-packages/huggingface_hub/file_download.py:979: UserWarning: `local_dir_use_symlinks` parameter is deprecated and will be ignored. The process to download files to a local folder has been updated and do not rely on symlinks anymore. You only need to pass a destination folder as`local_dir`.
For more details, check out https://huggingface.co/docs/huggingface_hub/main/en/guides/download#download-files-to-local-folder.
  warnings.warn(
[95m[1m======================================================================[0m
[95m[1müöÄ Jarvis IA V2 - Descarga de Modelos[0m
[95m[1m======================================================================[0m
[94m‚ÑπÔ∏è  Espacio libre en disco: 181 GB[0m
[92m‚úÖ Espacio en disco suficiente: 181GB[0m
[95m[1m======================================================================[0m
[95m[1müì¶ Categor√≠a: LLM_FLAGSHIP[0m
[95m[1m======================================================================[0m

[1/1] Qwen2.5-14B-Instruct-AWQ
  GPU: GPU0 (RTX 5070 Ti) | VRAM: 6-8GB
  üèÜ FLAGSHIP - √öNICO MODELO - El m√°s grande que cabe en RTX 5070 Ti
[94m‚ÑπÔ∏è  Descargando Qwen2.5-14B-Instruct-AWQ...[0m
[94m‚ÑπÔ∏è    Repositorio: Qwen/Qwen2.5-14B-Instruct-AWQ[0m
[94m‚ÑπÔ∏è    Tama√±o estimado: ~8GB[0m
[94m‚ÑπÔ∏è    Destino: models/llm/qwen2.5-14b-awq[0m
Fetching 13 files:   0%|          | 0/13 [00:00<?, ?it/s]Fetching 13 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 13/13 [00:00<00:00, 11383.29it/s]
[92m‚úÖ Qwen2.5-14B-Instruct-AWQ descargado correctamente[0m
[95m[1m======================================================================[0m
[95m[1müìä Resumen de Descarga[0m
[95m[1m======================================================================[0m
Total: 1
[92m‚úÖ Exitosos: 1[0m


[92m‚úÖ ‚ú® ¬°Todos los modelos descargados correctamente![0m
[94m‚ÑπÔ∏è  Pr√≥ximo paso: Ejecutar benchmarks y configurar el orquestador[0m
[95m[1m======================================================================[0m
[95m[1müìù Notas Importantes[0m
[95m[1m======================================================================[0m
1. Modelos de Llama requieren aceptar licencia en HuggingFace
2. Convierte Whisper a CTranslate2 para mejor rendimiento:
   ct2-transformers-converter --model models/whisper/large-v3-turbo \
     --output_dir models/whisper/large-v3-turbo-ct2 --quantization int8
3. Los modelos AWQ/GPTQ ya est√°n cuantizados (4-bit)
4. Espacio total usado: ~100-120GB
